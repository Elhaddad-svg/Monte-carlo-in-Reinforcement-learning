{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIpFteNJ0M1Q",
        "outputId": "3600cc65-6d61-4482-ae15-6d549cd76637"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (1.22.4)\n",
            "Collecting farama-notifications>=0.0.1\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (6.4.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (2.2.1)\n",
            "Collecting jax-jumpy>=1.0.0\n",
            "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.9/dist-packages (from gymnasium) (4.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.8.0->gymnasium) (3.15.0)\n",
            "Installing collected packages: farama-notifications, jax-jumpy, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_Tyeb-2wzSA"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import time # to get the time\n",
        "\n",
        "env = gym.make('MountainCar-v0',render_mode=\"rgb_array_list\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEQh_YUhwzSM",
        "outputId": "9d6ab559-fa72-4407-93e0-6637971dcda2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 20, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Set hyperparameters\n",
        "num_episodes = 500\n",
        "alpha = 0.1\n",
        "gamma = 0.95\n",
        "epsilon = 0.1\n",
        "\n",
        "# Set Q-table\n",
        "nA = env.action_space.n\n",
        "nS = env.observation_space.shape[0]\n",
        "np.random.seed(3)\n",
        "Observation = [20,20]\n",
        "Q = np.random.uniform(low=0, high=1, size=(Observation + [nA]))\n",
        "Q.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vj1JDyrOwzSO"
      },
      "outputs": [],
      "source": [
        "def get_discrete_state(state, n_bins=(20, 20)):\n",
        "    \"\"\"\n",
        "    Convert the continuous state values to discrete values.\n",
        "\n",
        "    Parameters:\n",
        "        state (np.ndarray): The current state of the environment.\n",
        "        n_bins (tuple): The number of bins to use for each state variable.\n",
        "\n",
        "    Returns:\n",
        "        tuple: The discrete state representation.\n",
        "    \"\"\"\n",
        "    # Define the bounds for each state variable\n",
        "    bounds = [\n",
        "        [-1.2, 0.5],  # cart position\n",
        "        [-0.07, 0.07],  # cart velocity\n",
        "    ]\n",
        "\n",
        "    # Calculate the bin width for each state variable\n",
        "    bin_widths = [(bounds[i][1] - bounds[i][0]) / n_bins[i] for i in range(len(bounds))]\n",
        "    # Convert each state variable to a discrete value\n",
        "    discrete_state = tuple(int((state[i] - bounds[i][0]) / bin_widths[i]) for i in range(len(bounds)))\n",
        "\n",
        "    # Make sure the discrete state is within the bounds of the Q-table\n",
        "    for i in range(len(bounds)):\n",
        "        if discrete_state[i] < 0:\n",
        "            discrete_state = list(discrete_state)\n",
        "            discrete_state[i] = 0\n",
        "            discrete_state = tuple(discrete_state)\n",
        "        elif discrete_state[i] >= n_bins[i]:\n",
        "            discrete_state = list(discrete_state)\n",
        "            discrete_state[i] = n_bins[i] - 1\n",
        "            discrete_state = tuple(discrete_state)\n",
        "\n",
        "    return discrete_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eEw2JwNwzSP"
      },
      "outputs": [],
      "source": [
        "# Define epsilon-greedy policy\n",
        "def epsilon_greedy(Q, state, nA, epsilon):\n",
        "    if np.random.random() > epsilon:\n",
        "        return np.argmax(Q[state])\n",
        "    else:\n",
        "        return np.random.choice(nA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_C_iTW25wzSQ",
        "outputId": "de5afe8b-8385-41a3-ce58-440832cf34be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 500/500"
          ]
        }
      ],
      "source": [
        "# MC algorithm\n",
        "total_reward =0\n",
        "total_time = 0\n",
        "episode_rewards = []\n",
        "episode_states = []\n",
        "episode_actions = []\n",
        "for i_episode in range(1,num_episodes+1):\n",
        "    total=0\n",
        "    episode_reward = []\n",
        "    episode_state = []\n",
        "    episode_action = []\n",
        "\n",
        "    t0 = time.time() #set the initial time\n",
        "    state = env.reset(seed=32)\n",
        "    d_state = get_discrete_state(state[0])\n",
        "    action = epsilon_greedy(Q, d_state, nA, epsilon)\n",
        "    terminated, truncated = False,False\n",
        "\n",
        "    while not (terminated or truncated):\n",
        "        next_state, reward, terminated, truncated, info = env.step(action)\n",
        "        if next_state[0] >= 0.5:\n",
        "            reward = 50\n",
        "        next_state = get_discrete_state(next_state)\n",
        "        episode_state.append(d_state)\n",
        "        episode_action.append(action)\n",
        "        episode_reward.append(reward)\n",
        "        next_action = epsilon_greedy(Q, next_state, nA, epsilon)\n",
        "        # Q[d_state][action] += alpha*(reward - Q[d_state][action])\n",
        "        d_state = next_state\n",
        "        action = next_action\n",
        "\n",
        "    episode_rewards.append(episode_reward)\n",
        "    episode_states.append(episode_state)\n",
        "    episode_actions.append(episode_action)\n",
        "    # Update Q table based on episode history\n",
        "    G = 0\n",
        "    # print(episode_rewards[i_episode])\n",
        "    for t in reversed(range(len(episode_rewards[i_episode-1]))):\n",
        "        G = gamma*G + episode_rewards[i_episode-1][t]\n",
        "        state = episode_states[i_episode-1][t]\n",
        "        action = episode_actions[i_episode-1][t]\n",
        "        Q[state[0], state[1], action] += (G - Q[state[0], state[1], action]) / (i_episode)\n",
        "\n",
        "    t1 = time.time() #episode has finished\n",
        "    episode_time = t1 - t0 #episode total time\n",
        "    total_time += episode_time\n",
        "    total_reward += sum(episode_reward) #episode total reward\n",
        "    # if i_episode % 10 == 0:\n",
        "    print(f\"\\rEpisode {i_episode}/{num_episodes}\", end=\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wctKdgjgwzSR",
        "outputId": "f2b6035e-7277-466a-949b-b040d83f7241"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Reward: -200.0\n",
            "Time Average: 0.8528874793052673\n"
          ]
        }
      ],
      "source": [
        "mean_reward = total_reward / num_episodes\n",
        "mean_time = total_time / num_episodes\n",
        "\n",
        "print(\"Mean Reward: \" + str(mean_reward))\n",
        "print(\"Time Average: \" + str(mean_time))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_Md1LCqwzSS",
        "outputId": "3a9f01da-1653-42ab-bcfd-ee76c90b9911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/videos/rl-video-episode-0.mp4.\n",
            "Moviepy - Writing video /content/videos/rl-video-episode-0.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/videos/rl-video-episode-0.mp4\n"
          ]
        }
      ],
      "source": [
        "from gymnasium.utils.save_video import save_video\n",
        "# Evaluate learned policy\n",
        "state = env.reset(seed=32)\n",
        "state = state[0]\n",
        "state = get_discrete_state(state)\n",
        "terminated, truncated = False,False\n",
        "video = []\n",
        "while not (terminated or truncated):\n",
        "    action = np.argmax(Q[state])\n",
        "    next_state, reward, terminated, truncated, info = env.step(action)\n",
        "    next_state = get_discrete_state(next_state)\n",
        "    state = next_state\n",
        "\n",
        "save_video(\n",
        "  env.render(),\n",
        "  \"videos\",\n",
        "  fps=30,\n",
        "  episode_index=0\n",
        ")\n",
        "\n",
        "# env.close()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}